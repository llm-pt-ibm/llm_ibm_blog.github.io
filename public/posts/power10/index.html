<!doctype html>

<html lang="pt-br">

<head><script src="/llm_ibm_blog.github.io/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=llm_ibm_blog.github.io/livereload" data-no-instant defer></script>
  <title>Realizando Inferências em CPU na Power10 - LLM IBM UFCG</title>
  <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name="description" content="The HTML5 Herald" />
<meta name="author" content="" /><meta property="og:url" content="http://localhost:1313/llm_ibm_blog.github.io/posts/power10/">
  <meta property="og:site_name" content="LLM IBM UFCG">
  <meta property="og:title" content="Realizando Inferências em CPU na Power10">
  <meta property="og:description" content="Realizando inferência em CPU utilizando a Power10.">
  <meta property="og:locale" content="pt_br">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-04-06T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-04-06T00:00:00+00:00">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="Power10">


  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Realizando Inferências em CPU na Power10">
  <meta name="twitter:description" content="Realizando inferência em CPU utilizando a Power10.">

<meta name="generator" content="Hugo 0.145.0">
    

  <link rel="stylesheet" href="/llm_ibm_blog.github.io/css/normalize.min.css" />
  <link rel="stylesheet" href="http://localhost:1313/llm_ibm_blog.github.io/fontawesome/css/all.min.css" />
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab|Ruda" />
  
  
  <link rel="stylesheet" type="text/css" href="/llm_ibm_blog.github.io/css/styles-light.css" />
  <link rel='stylesheet' href='http://localhost:1313/llm_ibm_blog.github.io/css/custom.css'><link rel='stylesheet' href='http://localhost:1313/llm_ibm_blog.github.io/css/font.css'>
  <style>
    .site-header {
      display: flex;
      flex-direction: row;
      align-items: center;
      justify-content: space-between;
      width: 100%;
      margin-bottom: 10px;
      margin-top: 15px;
    }
    .site-header .logo-wrapper {
      margin-left: 20px;  
    }
    .site-header .title-tagline-wrapper {
      display: flex;
      flex-direction: column;
      justify-content: center;
    }
    .site-header img,
    .site-header .logo-wrapper img,
    .logo-wrapper img,
    header img {
      max-height: 70px;
      width: auto;
      display: block;
      margin: 0;
      background-color: transparent;
      border: none;
      padding: 0;
      box-shadow: none !important;
      -webkit-box-shadow: none !important;
      -moz-box-shadow: none !important;
      filter: none !important;
      text-shadow: none !important;
    }
     
    header {
      margin-bottom: 0px;
    }
     
    header h1 {
      margin-bottom: 5px;  
      margin-top: 0;
    }
     
    .tagline {
      margin-top: 0;
      margin-bottom: 0;
    }
  </style>
</head>

<body>
  <div id="container">
    <header>
      <div class="site-header">
        <div class="title-tagline-wrapper">
          <h1>
            <a href="http://localhost:1313/llm_ibm_blog.github.io/">LLM IBM UFCG</a>
          </h1>
          
          <p class="tagline"><em>Blog com relatórios periódicos das tarefas realizadas.</em></p>
          
        </div>
        <div class="logo-wrapper">
          
<div class="site_logo featured_image">
    <a href="http://localhost:1313/llm_ibm_blog.github.io/">
        <img class="" 
             src="/llm_ibm_blog.github.io/site_logo.png">
    </a>
</div>


        </div>
      </div>

      <ul id="social-media">
      </ul>
    </header>

    
<nav>
    <ul>
        
        <li>
            <a class="" href="/llm_ibm_blog.github.io/">
                <i class="fa-li fa  fa-lg"></i><span>Início</span>
            </a>
        </li>
        
        <li>
            <a class="" href="/llm_ibm_blog.github.io/tags">
                <i class="fa-li fa  fa-lg"></i><span>Tags</span>
            </a>
        </li>
        
        <li>
            <a class="" href="/llm_ibm_blog.github.io/team/">
                <i class="fa-li fa  fa-lg"></i><span>Equipe</span>
            </a>
        </li>
        
    </ul>
</nav>


    <main>



<article>

    <h1>Realizando Inferências em CPU na Power10</h1>

    
      <aside>
    <ul>
        
        <li>
            <time class="post-date" datetime="2025-04-06T00:00:00Z">Apr 6, 2025</time>
        </li>
        
        

        
        
        <li>André Alves , Ronaldd Matias</li>
        

        
        

        
        <li>
            <em>
                
                    
                    <a href="/llm_ibm_blog.github.io/tags/llm">#LLM</a>
                
                    , 
                    <a href="/llm_ibm_blog.github.io/tags/power10">#Power10</a>
                
            </em>
        </li>
        

        <li></li>
    </ul>
</aside>
    

    
      

    

    <h2 id="contexto">Contexto</h2>
<p>Neste post iremos apresentar a nossa experiência em executar o modelo Granite-20b-Code-Instruct em uma máquina Power10, apresentando os desafios e demais configurações necessárias para realizar inferências utilizando o Llama.cpp, uma das bibliotecas opensource mais populares neste domínio.</p>
<h2 id="tldr">TLDR</h2>
<ul>
<li>Este post apresenta detalhes sobre como configurar e realizar inferências utilizando a infraestrutura da IBM Power 10;</li>
<li>Nosso maior desafio foi a configuração do Llama cpp, que demandou ajustes como a instalação do Ninja-builder, realização da compilação do OpenBLAS e atualização do compilador C.</li>
</ul>
<h2 id="infraestrutura">Infraestrutura</h2>
<p>As inferências foram realizadas em uma máquina com arquitetura IBM POWER10, equipada com 750GB de memória RAM e executando o sistema operacional Red Hat Enterprise Linux 8.10. O acesso ao ambiente é realizado por meio de uma VM, sendo necessário o uso de uma VPN para estabelecer uma comunicação segura e controlada com o sistema, possibilitando a execução das atividades de forma remota e eficiente.</p>
<h2 id="setup-inicial">Setup Inicial</h2>
<p>A biblioteca que nos permite executar LLMs utilizando os recursos computacionais da CPU é o Llama.cpp. Para a sua configuração, foi necessário resolver duas dependências externas: o Ninja-builder e o OpenBLAS. O NinjaBuilder é responsável por otimizar o processo de compulação, enquanto o OpenBLAS é uma biblioteca responsável pelos cálculos matriciais de alto desempenho.</p>
<p>Durante o processo de build do OpenBLAS, identificamos discrepâncias nos testes internos de validação dos cálculos matriciais, indicando um problema de compatibilidade com o compilador C disponível, que estava em uma versão mais antiga, a 8.5.0. A solução, portanto, <strong>foi a atualização do compilador para uma versão mais recente, a 13.2</strong>, garantindo melhor compatibilidade com a arquitetura Power10 e validando a precisão das operações numéricas necessárias para o funcionamento do Llama.cpp. A seguir, apresentamos o passo a passo realizado para viabilizar a compilação das bibliotecas necessárias, bem como a atualização do compilador C.</p>
<ol>
<li>Criando o ambiente de compilação para o builder</li>
</ol>
<pre tabindex="0"><code>sudo dnf update -y  &amp;&amp; dnf -y groupinstall &#39;Development Tools&#39; &amp;&amp; dnf install -y \ cmake git ninja-build-debugsource.ppc64le \ &amp;&amp; dnf clean all
</code></pre><ol start="2">
<li>Atualizando compilador C e definindo variáveis de ambiente</li>
</ol>
<pre tabindex="0"><code>scl enable gcc-toolset-13 bash
export CC=/usr/bin/gcc-13
export CXX=/usr/bin/g++-13
</code></pre><ol start="3">
<li>Baixando e compilando o OpenBLAS</li>
</ol>
<pre tabindex="0"><code>git clone --recursive https://github.com/DanielCasali/OpenBLAS.git &amp;&amp; cd OpenBLAS &amp;&amp; \ make -j$(nproc --all) TARGET=POWER10 DYNAMIC_ARCH=1 &amp;&amp; \ make PREFIX=/opt/OpenBLAS install &amp;&amp; \ cd /
</code></pre><ol start="4">
<li>Baixando e compilando o Llama.cpp usando a biblioteca OpenBLAS que acabamos de baixar</li>
</ol>
<pre tabindex="0"><code> git clone https://github.com/DanielCasali/llama.cpp.git &amp;&amp; cd llama.cpp &amp;&amp; sed -i &#34;s/powerpc64le/native -mvsx -mtune=native -D__POWER10_VECTOR__/g&#34; ggml/src/CMakeLists.txt &amp;&amp; \ mkdir build; \ cd build; \ cmake -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS -DBLAS_INCLUDE_DIRS=/opt/OpenBLAS/include -G Ninja ..; \ cmake --build . --config Release
</code></pre><p>Com todos esses passos realizados com sucesso, o ambiente foi devidamente configurado e otimizado para a execução local do Llama.cpp. Agora, somos capazes de iniciar um servidor para realizar inferências com LLM&rsquo;s de forma eficiente, utilizando exclusivamente os recursos da CPU.</p>
<h2 id="realizando-inferência">Realizando Inferência</h2>
<p>Nós escolhemos o modelo Granite-20b-code-instruct no formato .GGUF, que é desenvolvido especificamente para otimizar o desempenho de modelos de linguagem em ambientes que utilizam apenas CPU. Esses modelos são quantizados, ou seja, a precisão dos cálculos feitos por eles são reduzidas, e, por conseguinte, o tamanho e consumo de memória também são menores, tornando-os ideais para a execução eficiente com Llama.cpp. Essa abordagem viabiliza inferências locais com alto desempenho, mesmo em arquiteturas baseadas exclusivamente em processadores, como é o caso da POWER10.
O load do modelo foi feito diretamente do Hugging Face. A seguir, mostraremos o passo a passo para realizar o download:</p>
<ol>
<li>Criar um diretório para o modelo no Llama.cpp:</li>
</ol>
<pre tabindex="0"><code>mkdir -p /root/llama.cpp/models/granite-20b-code-instruct-8k-GGUF
</code></pre><ol start="2">
<li>Acessar o diretório no Llama.cpp:</li>
</ol>
<pre tabindex="0"><code>cd /root/llama.cpp/models/granite-20b-code-instruct-8k-GGUF
</code></pre><ol start="3">
<li>Baixar o modelo via Hugging Face:</li>
</ol>
<pre tabindex="0"><code>wget https://huggingface.co/ibm-granite/granite-20b-code-instruct-8k-GGUF/resolve/main/granite-20b-code-instruct.Q4_K_M.gguf
</code></pre><p>O último passo pode ser mais demorado a depender da quantidade de parâmetros do modelo. Todavia, após concluir os passos acima, podemos subir um servidor Llama.cpp para que seja possível realizarmos inferências, por padrão, o servidor é exposto na porta 8080 da Power10, mas isso é completamente customizável. O código a seguir ilustra como configurar e executar o servidor Llama:</p>
<pre tabindex="0"><code>/root/llama.cpp/build/bin/llama-server --host 0.0.0.0 --model /root/llama.cpp/models/granite-20b-code-instruct-8k-GGUF/granite-20b-code-instruct.Q4_K_M.gguf
</code></pre><p>Com o servidor do Llama server executando na porta 8080, agora somos capazes de realizar inferências via requisições HTTP. Neste exemplo, para fins de simplicidade, utilizamos o curl para requisições:</p>
<pre tabindex="0"><code>curl -X POST http://localhost:8080/completion \
     -H &#34;Content-Type: application/json&#34; \
     -d &#39;{
           &#34;prompt&#34;: &#34;Make a hello world program in Java. Your answer should be in Java code only.&#34;,
           &#34;max_tokens&#34;: 100
         }&#39;
</code></pre><p>A seguir, um exemplo de como a resposta é retornada:</p>
<pre tabindex="0"><code>{
  &#34;content&#34;: &#34;
public class HelloWorld {
    public static void main(String[] args) {
        System.out.println(&#34;Hello, World!&#34;);
    }
}
</code></pre><p>Com isso, agora somos capazes de realizar inferências em CPU. Nossos próximos passos visa realizar essa inferências utilizando o Framework de avaliação HELM (Holistic Evaluation of Language Models) como mediador.</p>


</article>


<section class="post-nav">
    <ul>
        <li>
        
            <a href="http://localhost:1313/llm_ibm_blog.github.io/posts/introducao/"><i class="fa fa-chevron-circle-left"></i> Introdução</a>
        
        </li>
        <li>
        
        </li>
    </ul>
</section>
  
    
    
  





<footer>
    <ul>
        <li>
            <h6>IBM &amp; UFCG - 2025 
                <a href="http://localhost:1313/llm_ibm_blog.github.io/index.xml"> </a></h6>
        </li>
        
        
    </ul>
</footer>
