
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
 <channel>
   <title>Posts on LLM IBM UFCG</title>
   <link>https://llm-pt-ibm.github.io/posts/</link>
   <description>Recent content in Posts on LLM IBM UFCG</description>
   <generator>Hugo -- gohugo.io</generator>
   <language>pt-br</language>
   <copyright>IBM &amp; UFCG - 2025</copyright>
   <lastBuildDate>Sun, 06 Apr 2025 00:00:00 +0000</lastBuildDate>
   
       <atom:link href="https://llm-pt-ibm.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
   
   
     <item>
       <title>Realizando Inferências em CPU na Power10</title>
       <link>https://llm-pt-ibm.github.io/posts/power10/</link>
       <pubDate>Sun, 06 Apr 2025 00:00:00 +0000</pubDate>
       
       <guid>https://llm-pt-ibm.github.io/posts/power10/</guid>
       <description>&lt;h2 id=&#34;contexto&#34;&gt;Contexto&lt;/h2&gt;&lt;p&gt;Neste post iremos apresentar a nossa experiência em executar o modelo Granite-20b-Code-Instruct em uma máquina Power10, apresentando os desafios e demais configurações necessárias para realizar inferências utilizando o Llama.cpp, uma das bibliotecas opensource mais populares neste domínio.&lt;/p&gt;&lt;h2 id=&#34;tldr&#34;&gt;TLDR&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Este post apresenta detalhes sobre como configurar e realizar inferências utilizando a infraestrutura da IBM Power 10;&lt;/li&gt;&lt;li&gt;Nosso maior desafio foi a configuração do Llama cpp, que demandou ajustes como a instalação do Ninja-builder, realização da compilação do OpenBLAS e atualização do compilador C.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&#34;infraestrutura&#34;&gt;Infraestrutura&lt;/h2&gt;&lt;p&gt;As inferências foram realizadas em uma máquina com arquitetura IBM POWER10, equipada com 750GB de memória RAM e executando o sistema operacional Red Hat Enterprise Linux 8.10. O acesso ao ambiente é realizado por meio de uma VM, sendo necessário o uso de uma VPN para estabelecer uma comunicação segura e controlada com o sistema, possibilitando a execução das atividades de forma remota e eficiente.&lt;/p&gt;&lt;h2 id=&#34;setup-inicial&#34;&gt;Setup Inicial&lt;/h2&gt;&lt;p&gt;A biblioteca que nos permite executar LLMs utilizando os recursos computacionais da CPU é o Llama.cpp. Para a sua configuração, foi necessário resolver duas dependências externas: o Ninja-builder e o OpenBLAS. O NinjaBuilder é responsável por otimizar o processo de compilação, enquanto o OpenBLAS é uma biblioteca responsável pelos cálculos matriciais de alto desempenho.&lt;/p&gt;&lt;p&gt;Durante o processo de build do OpenBLAS, identificamos discrepâncias nos testes internos de validação dos cálculos matriciais, indicando um problema de compatibilidade com o compilador C disponível, que estava em uma versão mais antiga, a 8.5.0. A solução, portanto, &lt;strong&gt;foi a atualização do compilador para uma versão mais recente, a 13.2&lt;/strong&gt;, garantindo melhor compatibilidade com a arquitetura Power10 e validando a precisão das operações numéricas necessárias para o funcionamento do Llama.cpp. A seguir, apresentamos o passo a passo realizado para viabilizar a compilação das bibliotecas necessárias, bem como a atualização do compilador C.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Criando o ambiente de compilação para o builder&lt;/li&gt;&lt;/ol&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo dnf update -y  &amp;amp;&amp;amp; dnf -y groupinstall &amp;#39;Development Tools&amp;#39; &amp;amp;&amp;amp; dnf install -y \ cmake git ninja-build-debugsource.ppc64le \ &amp;amp;&amp;amp; dnf clean all&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;&lt;li&gt;Atualizando compilador C e definindo variáveis de ambiente&lt;/li&gt;&lt;/ol&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;scl enable gcc-toolset-13 bashexport CC=/usr/bin/gcc-13export CXX=/usr/bin/g++-13&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;&lt;li&gt;Baixando e compilando o OpenBLAS&lt;/li&gt;&lt;/ol&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;git clone --recursive https://github.com/DanielCasali/OpenBLAS.git &amp;amp;&amp;amp; cd OpenBLAS &amp;amp;&amp;amp; \ make -j$(nproc --all) TARGET=POWER10 DYNAMIC_ARCH=1 &amp;amp;&amp;amp; \ make PREFIX=/opt/OpenBLAS install &amp;amp;&amp;amp; \ cd /&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;&lt;li&gt;Baixando e compilando o Llama.cpp usando a biblioteca OpenBLAS que acabamos de baixar&lt;/li&gt;&lt;/ol&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt; git clone https://github.com/DanielCasali/llama.cpp.git &amp;amp;&amp;amp; cd llama.cpp &amp;amp;&amp;amp; sed -i &amp;#34;s/powerpc64le/native -mvsx -mtune=native -D__POWER10_VECTOR__/g&amp;#34; ggml/src/CMakeLists.txt &amp;amp;&amp;amp; \ mkdir build; \ cd build; \ cmake -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS -DBLAS_INCLUDE_DIRS=/opt/OpenBLAS/include -G Ninja ..; \ cmake --build . --config Release&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Com todos esses passos realizados com sucesso, o ambiente foi devidamente configurado e otimizado para a execução local do Llama.cpp. Agora, somos capazes de iniciar um servidor para realizar inferências com LLM&amp;rsquo;s de forma eficiente, utilizando exclusivamente os recursos da CPU.&lt;/p&gt;&lt;h2 id=&#34;realizando-inferência&#34;&gt;Realizando Inferência&lt;/h2&gt;&lt;p&gt;Nós escolhemos o modelo Granite-20b-code-instruct no formato .GGUF, que é desenvolvido especificamente para otimizar o desempenho de modelos de linguagem em ambientes que utilizam apenas CPU. Esses modelos são quantizados, ou seja, a precisão dos cálculos feitos por eles são reduzidas, e, por conseguinte, o tamanho e consumo de memória também são menores, tornando-os ideais para a execução eficiente com Llama.cpp. Essa abordagem viabiliza inferências locais com alto desempenho, mesmo em arquiteturas baseadas exclusivamente em processadores, como é o caso da POWER10.O download do modelo foi feito diretamente do Hugging Face. A seguir, mostraremos o passo a passo para realizar o download:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Criar um diretório para o modelo no Llama.cpp:&lt;/li&gt;&lt;/ol&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;mkdir -p /root/llama.cpp/models/granite-20b-code-instruct-8k-GGUF&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;&lt;li&gt;Acessar o diretório no Llama.cpp:&lt;/li&gt;&lt;/ol&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cd /root/llama.cpp/models/granite-20b-code-instruct-8k-GGUF&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;&lt;li&gt;Baixar o modelo via Hugging Face:&lt;/li&gt;&lt;/ol&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;wget https://huggingface.co/ibm-granite/granite-20b-code-instruct-8k-GGUF/resolve/main/granite-20b-code-instruct.Q4_K_M.gguf&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;O último passo pode ser mais demorado a depender da quantidade de parâmetros do modelo. Todavia, após concluir os passos acima, podemos subir um servidor Llama.cpp para que seja possível realizarmos inferências, por padrão, o servidor é exposto na porta 8080 da Power10, mas isso é completamente customizável. O código a seguir ilustra como configurar e executar o servidor Llama:&lt;/p&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;/root/llama.cpp/build/bin/llama-server --host 0.0.0.0 --model /root/llama.cpp/models/granite-20b-code-instruct-8k-GGUF/granite-20b-code-instruct.Q4_K_M.gguf&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Com o servidor do Llama.cpp executando na porta 8080, agora somos capazes de realizar inferências via requisições HTTP. Neste exemplo, para fins de simplicidade, utilizamos o curl para requisições:&lt;/p&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl -X POST http://localhost:8080/completion \     -H &amp;#34;Content-Type: application/json&amp;#34; \     -d &amp;#39;{           &amp;#34;prompt&amp;#34;: &amp;#34;Make a hello world program in Java. Your answer should be in Java code only.&amp;#34;,           &amp;#34;max_tokens&amp;#34;: 100         }&amp;#39;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A seguir, um exemplo de como a resposta é retornada:&lt;/p&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;{  &amp;#34;content&amp;#34;: &amp;#34;public class HelloWorld {    public static void main(String[] args) {        System.out.println(&amp;#34;Hello, World!&amp;#34;);    }}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Com isso, agora somos capazes de realizar inferências em CPU. Nossos próximos passos visa realizar essas inferências utilizando o &lt;em&gt;Framework&lt;/em&gt; de avaliação HELM (&lt;em&gt;Holistic Evaluation of Language Models&lt;/em&gt;) como mediador.&lt;/p&gt;</description>
     </item>
   
     <item>
       <title>Introdução</title>
       <link>https://llm-pt-ibm.github.io/posts/introducao/</link>
       <pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate>
       
       <guid>https://llm-pt-ibm.github.io/posts/introducao/</guid>
       <description>&lt;p&gt;Bem-vindo ao blog do projeto LLM-PT-IBM. Aqui, serão apresentados relatórios e resultados do projeto desenvolvido em parceria entre a Universidade Federal de Campina Grande (UFCG) e a IBM, com foco na avaliação de modelos de linguagem para a língua portuguesa. Assim será possível acompanhar atualizações sobre os avanços e análises realizadas ao longo do projeto.&lt;/p&gt;</description>
     </item>
   
 </channel>
</rss>
